{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGGXxvhbbFKk"
      },
      "source": [
        "# Medidas de Desempeño"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJRuM2DBbLuv"
      },
      "source": [
        "## Parte 1. Sin Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTuaFeLUZ9DL",
        "outputId": "0753594c-3f4f-4365-e6c0-dc34e8a38601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.625\n",
            "Error: 0.375\n",
            "Confusion Matrix: [[3, 2], [1, 2]]\n",
            "Precision: 0.6\n",
            "Recall: 0.75\n",
            "Positive Predictive Value: 0.6\n",
            "True Positive Rate: 0.75\n",
            "True Negative Rate: 0.5\n",
            "False Positive Rate: 0.5\n",
            "False Negative Rate: 0.25\n",
            "F1-Score: 0.6666666666666665\n"
          ]
        }
      ],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calcula la precisión (accuracy).\n",
        "\n",
        "    Args:\n",
        "        y_true: Lista/tupla de valores reales.\n",
        "        y_pred: Lista/tupla de valores predichos.\n",
        "\n",
        "    Returns:\n",
        "        La precisión como un flotante.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    for i in range(len(y_true)):\n",
        "        if y_true[i] == y_pred[i]:\n",
        "            correct += 1\n",
        "    return correct / len(y_true)\n",
        "\n",
        "\n",
        "def error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calcula el error.\n",
        "\n",
        "    Args:\n",
        "        y_true: Lista/tupla de valores reales.\n",
        "        y_pred: Lista/tupla de valores predichos.\n",
        "\n",
        "    Returns:\n",
        "        El error como un flotante.\n",
        "    \"\"\"\n",
        "    return 1 - accuracy(y_true, y_pred)\n",
        "\n",
        "\n",
        "def confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calcula la matriz de confusión para un problema de clasificación binaria.\n",
        "\n",
        "    Args:\n",
        "        y_true: Lista/tupla de valores reales.\n",
        "        y_pred: Lista/tupla de valores predichos.\n",
        "\n",
        "    Returns:\n",
        "        Una lista de listas que representa la matriz de confusión:\n",
        "        [[TP, FP], [FN, TN]]\n",
        "    \"\"\"\n",
        "    TP = 0\n",
        "    FP = 0\n",
        "    FN = 0\n",
        "    TN = 0\n",
        "\n",
        "    for i in range(len(y_true)):\n",
        "        if y_true[i] == 1 and y_pred[i] == 1:\n",
        "            TP += 1\n",
        "        elif y_true[i] == 0 and y_pred[i] == 1:\n",
        "            FP += 1\n",
        "        elif y_true[i] == 1 and y_pred[i] == 0:\n",
        "            FN += 1\n",
        "        elif y_true[i] == 0 and y_pred[i] == 0:\n",
        "            TN += 1\n",
        "\n",
        "    return [[TP, FP], [FN, TN]]\n",
        "\n",
        "\n",
        "def precision(TP, FP):\n",
        "  \"\"\"Calcula la precisión.\"\"\"\n",
        "  if TP + FP == 0:  # Manejo de la división por cero\n",
        "      return 0.0\n",
        "  return TP / (TP + FP)\n",
        "\n",
        "def recall(TP, FN):\n",
        "  \"\"\"Calcula el recall (sensibilidad).\"\"\"\n",
        "  if TP + FN == 0: # Manejo de la división por cero\n",
        "      return 0.0\n",
        "  return TP / (TP + FN)\n",
        "\n",
        "\n",
        "def positive_predictive_value(TP, FP):\n",
        "    \"\"\"Calcula el valor predictivo positivo (igual a la precisión).\"\"\"\n",
        "    return precision(TP, FP)  # Es lo mismo que precisión\n",
        "\n",
        "\n",
        "def true_positive_rate(TP, FN):\n",
        "    \"\"\"Calcula la tasa de verdaderos positivos (igual a recall).\"\"\"\n",
        "    return recall(TP, FN) # es lo mismo que recall\n",
        "\n",
        "\n",
        "def true_negative_rate(TN, FP):\n",
        "    \"\"\"Calcula la tasa de verdaderos negativos (especificidad).\"\"\"\n",
        "    if TN + FP == 0:  # Manejo de la división por cero\n",
        "      return 0.0\n",
        "    return TN / (TN + FP)\n",
        "\n",
        "\n",
        "def false_positive_rate(FP, TN):\n",
        "    \"\"\"Calcula la tasa de falsos positivos.\"\"\"\n",
        "    if FP + TN == 0: # Manejo de la división por cero\n",
        "      return 0.0\n",
        "\n",
        "    return FP / (FP + TN)\n",
        "\n",
        "\n",
        "def false_negative_rate(FN, TP):\n",
        "    \"\"\"Calcula la tasa de falsos negativos.\"\"\"\n",
        "    if FN + TP == 0: # Manejo de la división por cero\n",
        "        return 0.0\n",
        "    return FN / (FN + TP)\n",
        "\n",
        "\n",
        "def f1_score(TP, FP, FN):\n",
        "    \"\"\"Calcula la puntuación F1.\"\"\"\n",
        "    p = precision(TP, FP)\n",
        "    r = recall(TP, FN)\n",
        "\n",
        "    if p + r == 0: # Manejo de la división por cero\n",
        "      return 0.0\n",
        "    return 2 * p * r / (p + r)\n",
        "\n",
        "\n",
        "# Ejemplo:\n",
        "y_true = [1, 0, 1, 1, 0, 0, 1, 0]\n",
        "y_pred = [1, 1, 0, 1, 0, 1, 1, 0]\n",
        "\n",
        "print(\"Accuracy:\", accuracy(y_true, y_pred))\n",
        "print(\"Error:\", error(y_true, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\", cm)\n",
        "\n",
        "TP = cm[0][0]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "TN = cm[1][1]\n",
        "\n",
        "\n",
        "print(\"Precision:\", precision(TP, FP))\n",
        "print(\"Recall:\", recall(TP, FN))\n",
        "print(\"Positive Predictive Value:\", positive_predictive_value(TP, FP))\n",
        "print(\"True Positive Rate:\", true_positive_rate(TP, FN))\n",
        "print(\"True Negative Rate:\", true_negative_rate(TN, FP))\n",
        "print(\"False Positive Rate:\", false_positive_rate(FP, TN))\n",
        "print(\"False Negative Rate:\", false_negative_rate(FN, TP))\n",
        "print(\"F1-Score:\", f1_score(TP, FP, FN))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow7VGfVIc4ju"
      },
      "source": [
        "## Parte 2. Utilizando scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LkVSYk9dDxJ",
        "outputId": "2d0007d9-d4b8-477b-8da1-cdff238287c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusión (sklearn):\n",
            " [[2 2]\n",
            " [1 3]]\n",
            "Accuracy (sklearn): 0.625\n",
            "Precision (sklearn): 0.6\n",
            "Recall (sklearn): 0.75\n",
            "F1-score (sklearn): 0.6666666666666666\n",
            "\n",
            "Reporte de Clasificación (sklearn):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.50      0.57         4\n",
            "           1       0.60      0.75      0.67         4\n",
            "\n",
            "    accuracy                           0.62         8\n",
            "   macro avg       0.63      0.62      0.62         8\n",
            "weighted avg       0.63      0.62      0.62         8\n",
            "\n",
            "True Positive Rate (manual): 0.6666666666666666\n",
            "True Negative Rate (manual): 0.6\n",
            "False Positive Rate (manual): 0.4\n",
            "False Negative Rate (manual): 0.3333333333333333\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Ejemplo de datos (los mismos que usamos antes)\n",
        "y_true = np.array([1, 0, 1, 1, 0, 0, 1, 0])\n",
        "y_pred = np.array([1, 1, 0, 1, 0, 1, 1, 0])\n",
        "\n",
        "\n",
        "# Matriz de confusión\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Matriz de Confusión (sklearn):\\n\", cm)\n",
        "\n",
        "# Accuracy (precisión)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy (sklearn):\", accuracy)\n",
        "\n",
        "# Precision (precisión) -  Para la clase positiva (1 en este caso)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "print(\"Precision (sklearn):\", precision)\n",
        "\n",
        "# Recall (sensibilidad o tasa de verdaderos positivos) - Para la clase positiva (1)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "print(\"Recall (sklearn):\", recall)\n",
        "\n",
        "\n",
        "# F1-score - Para la clase positiva (1)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "print(\"F1-score (sklearn):\", f1)\n",
        "\n",
        "\n",
        "# Reporte de clasificación (incluye precision, recall, f1-score, support)\n",
        "print(\"\\nReporte de Clasificación (sklearn):\\n\", classification_report(y_true, y_pred))\n",
        "\n",
        "\n",
        "# Para métricas como Tasa de Verdaderos Negativos (TNR) o Tasa de Falsos Positivos (FPR)\n",
        "TN = cm[0, 0]\n",
        "FP = cm[0, 1]\n",
        "FN = cm[1, 0]\n",
        "TP = cm[1, 1]\n",
        "\n",
        "\n",
        "\n",
        "#Aquí se ajusta el calculo para que coincida con el calculo de la matriz anterior\n",
        "TN = cm[1, 1]\n",
        "FP = cm[0, 1]\n",
        "FN = cm[1, 0]\n",
        "TP = cm[0, 0]\n",
        "\n",
        "\n",
        "tpr = TP / (TP + FN) #Igual a recall\n",
        "tnr = TN / (TN + FP)\n",
        "fpr = FP / (FP + TN)\n",
        "fnr = FN / (TP + FN)\n",
        "\n",
        "\n",
        "print(\"True Positive Rate (manual):\", tpr)\n",
        "print(\"True Negative Rate (manual):\", tnr)\n",
        "print(\"False Positive Rate (manual):\", fpr)\n",
        "print(\"False Negative Rate (manual):\", fnr)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
