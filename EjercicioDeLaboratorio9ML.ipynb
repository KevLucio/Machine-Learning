{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE y Perceptron Simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados antes de SMOTE:\n",
      "\n",
      "Clasificador Euclidiano:\n",
      "Hold-out accuracy: 0.5581\n",
      "CV accuracy: 0.4522 ± 0.0737\n",
      "\n",
      "1-NN:\n",
      "Hold-out accuracy: 0.8372\n",
      "CV accuracy: 0.7195 ± 0.0947\n",
      "\n",
      "Resultados después de SMOTE:\n",
      "\n",
      "Clasificador Euclidiano:\n",
      "Hold-out accuracy: 0.6140\n",
      "CV accuracy: 0.5622 ± 0.0736\n",
      "\n",
      "1-NN:\n",
      "Hold-out accuracy: 0.8772\n",
      "CV accuracy: 0.8042 ± 0.0922\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "class EuclideanClassifier:\n",
    "    def __init__(self):\n",
    "        self.centroids = {}\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        classes = np.unique(y)\n",
    "        for c in classes:\n",
    "            # Calcular el centroide para cada clase\n",
    "            self.centroids[c] = np.mean(X[y == c], axis=0)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = {c: euclidean_distance(x, centroid) \n",
    "                        for c, centroid in self.centroids.items()}\n",
    "            predictions.append(min(distances.items(), key=lambda x: x[1])[0])\n",
    "        return np.array(predictions)\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=1):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.y_train[k_indices]\n",
    "            most_common = np.bincount(k_nearest_labels).argmax()\n",
    "            predictions.append(most_common)\n",
    "        return np.array(predictions)\n",
    "\n",
    "def smote(X, y, k=5, minority_target=None, n_synthetic=None):\n",
    "    \"\"\"\n",
    "    Implementación de SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "    \"\"\"\n",
    "    # Identificar la clase minoritaria si no se especifica\n",
    "    if minority_target is None:\n",
    "        counts = Counter(y)\n",
    "        minority_target = min(counts, key=counts.get)\n",
    "    \n",
    "    # Obtener ejemplos de la clase minoritaria\n",
    "    minority_indices = np.where(y == minority_target)[0]\n",
    "    X_minority = X[minority_indices]\n",
    "    \n",
    "    if n_synthetic is None:\n",
    "        # Calcular cuántas muestras sintéticas generar\n",
    "        counts = Counter(y)\n",
    "        n_synthetic = max(counts.values()) - counts[minority_target]\n",
    "    \n",
    "    synthetic_samples = []\n",
    "    \n",
    "    # Generar muestras sintéticas\n",
    "    for _ in range(n_synthetic):\n",
    "        # Seleccionar un ejemplo de la clase minoritaria al azar\n",
    "        idx = np.random.randint(0, len(X_minority))\n",
    "        point = X_minority[idx]\n",
    "        \n",
    "        # Encontrar k vecinos más cercanos\n",
    "        distances = [euclidean_distance(point, p) for p in X_minority]\n",
    "        sorted_indices = np.argsort(distances)[1:k+1]  # Excluir el punto mismo\n",
    "        \n",
    "        # Seleccionar un vecino al azar\n",
    "        neighbor_idx = np.random.choice(sorted_indices)\n",
    "        neighbor = X_minority[neighbor_idx]\n",
    "        \n",
    "        # Generar punto sintético\n",
    "        diff = neighbor - point\n",
    "        gap = np.random.random()\n",
    "        synthetic_point = point + gap * diff\n",
    "        \n",
    "        synthetic_samples.append(synthetic_point)\n",
    "    \n",
    "    # Combinar datos originales con sintéticos\n",
    "    X_synthetic = np.vstack((X, synthetic_samples))\n",
    "    y_synthetic = np.hstack((y, [minority_target] * n_synthetic))\n",
    "    \n",
    "    return X_synthetic, y_synthetic\n",
    "\n",
    "def evaluate_classifier(clf, X, y, test_size=0.2, cv_folds=10):\n",
    "    \"\"\"\n",
    "    Evalúa el clasificador usando both hold-out y k-fold cross-validation\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Hold-out validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    holdout_accuracy = np.mean(y_pred == y_test)\n",
    "    results['holdout'] = holdout_accuracy\n",
    "    \n",
    "    # K-fold cross-validation\n",
    "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        cv_scores.append(np.mean(y_pred == y_test))\n",
    "    \n",
    "    results['cv_mean'] = np.mean(cv_scores)\n",
    "    results['cv_std'] = np.std(cv_scores)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Cargar y preparar los datos\n",
    "glass_identification = fetch_ucirepo(id=42)\n",
    "X = glass_identification.data.features.values\n",
    "y = glass_identification.data.targets.values.ravel()\n",
    "\n",
    "# Crear instancias de los clasificadores\n",
    "euclidean_clf = EuclideanClassifier()\n",
    "knn_clf = KNNClassifier(k=1)\n",
    "\n",
    "# Evaluar antes de SMOTE\n",
    "print(\"Resultados antes de SMOTE:\")\n",
    "print(\"\\nClasificador Euclidiano:\")\n",
    "euclidean_results_before = evaluate_classifier(euclidean_clf, X, y)\n",
    "print(f\"Hold-out accuracy: {euclidean_results_before['holdout']:.4f}\")\n",
    "print(f\"CV accuracy: {euclidean_results_before['cv_mean']:.4f} ± {euclidean_results_before['cv_std']:.4f}\")\n",
    "\n",
    "print(\"\\n1-NN:\")\n",
    "knn_results_before = evaluate_classifier(knn_clf, X, y)\n",
    "print(f\"Hold-out accuracy: {knn_results_before['holdout']:.4f}\")\n",
    "print(f\"CV accuracy: {knn_results_before['cv_mean']:.4f} ± {knn_results_before['cv_std']:.4f}\")\n",
    "\n",
    "# Aplicar SMOTE\n",
    "X_balanced, y_balanced = smote(X, y)\n",
    "\n",
    "# Evaluar después de SMOTE\n",
    "print(\"\\nResultados después de SMOTE:\")\n",
    "print(\"\\nClasificador Euclidiano:\")\n",
    "euclidean_results_after = evaluate_classifier(euclidean_clf, X_balanced, y_balanced)\n",
    "print(f\"Hold-out accuracy: {euclidean_results_after['holdout']:.4f}\")\n",
    "print(f\"CV accuracy: {euclidean_results_after['cv_mean']:.4f} ± {euclidean_results_after['cv_std']:.4f}\")\n",
    "\n",
    "print(\"\\n1-NN:\")\n",
    "knn_results_after = evaluate_classifier(knn_clf, X_balanced, y_balanced)\n",
    "print(f\"Hold-out accuracy: {knn_results_after['holdout']:.4f}\")\n",
    "print(f\"CV accuracy: {knn_results_after['cv_mean']:.4f} ± {knn_results_after['cv_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X: (150, 4)\n",
      "Forma de y: (150,)\n",
      "Valores únicos en y: [0 1]\n",
      "Distribución de clases: [100  50]\n",
      "\n",
      "Conjuntos de entrenamiento y prueba:\n",
      "X_train shape: (105, 4)\n",
      "X_test shape: (45, 4)\n",
      "y_train shape: (105,)\n",
      "y_test shape: (45,)\n",
      "\n",
      "Resultados en conjunto de entrenamiento:\n",
      "Accuracy: 0.9524\n",
      "Precision: 0.9655\n",
      "Recall: 0.8750\n",
      "F1-Score: 0.9180\n",
      "Matriz de Confusión:\n",
      "TP: 28, TN: 72\n",
      "FP: 1, FN: 4\n",
      "\n",
      "Resultados en conjunto de prueba:\n",
      "Accuracy: 0.9556\n",
      "Precision: 1.0000\n",
      "Recall: 0.8889\n",
      "F1-Score: 0.9412\n",
      "Matriz de Confusión:\n",
      "TP: 16, TN: 27\n",
      "FP: 0, FN: 2\n",
      "\n",
      "Pesos finales: [-0.48193779 -0.25098156  1.88421894  1.00312574]\n",
      "Bias final: -2.0000000000000004\n",
      "Iteraciones hasta convergencia: 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "class SimplePerceptron:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Inicializar pesos con valores aleatorios pequeños\n",
    "        self.weights = np.random.randn(n_features) * 0.01\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Historial de errores para monitoreo\n",
    "        self.errors = []\n",
    "        \n",
    "        # Entrenamiento\n",
    "        for _ in range(self.n_iterations):\n",
    "            errors = 0\n",
    "            \n",
    "            for idx in range(n_samples):\n",
    "                linear_output = np.dot(X[idx], self.weights) + self.bias\n",
    "                prediction = self.activation_function(linear_output)\n",
    "                error = y[idx] - prediction\n",
    "                \n",
    "                # Actualizar pesos y bias\n",
    "                self.weights += self.learning_rate * error * X[idx]\n",
    "                self.bias += self.learning_rate * error\n",
    "                errors += abs(error)\n",
    "            \n",
    "            self.errors.append(errors)\n",
    "            if errors == 0:\n",
    "                break\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return np.array([self.activation_function(output) for output in linear_output])\n",
    "    \n",
    "    def activation_function(self, x):\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calcula métricas de evaluación manualmente\"\"\"\n",
    "    if len(y_true) == 0:\n",
    "        return 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    # Calcular matriz de confusión\n",
    "    tn = sum((yt == 0 and yp == 0) for yt, yp in zip(y_true, y_pred))\n",
    "    fp = sum((yt == 0 and yp == 1) for yt, yp in zip(y_true, y_pred))\n",
    "    fn = sum((yt == 1 and yp == 0) for yt, yp in zip(y_true, y_pred))\n",
    "    tp = sum((yt == 1 and yp == 1) for yt, yp in zip(y_true, y_pred))\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = (tp + tn) / len(y_true) if len(y_true) > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall, f1, tp, tn, fp, fn\n",
    "\n",
    "# Cargar y preparar los datos\n",
    "iris = fetch_ucirepo(id=53)\n",
    "X_raw = iris.data.features.values\n",
    "y_raw = iris.data.targets.values.ravel()\n",
    "\n",
    "# Convertir etiquetas string a números\n",
    "y_numeric = np.zeros_like(y_raw, dtype=int)\n",
    "y_numeric[y_raw == 'Iris-setosa'] = 0\n",
    "y_numeric[y_raw == 'Iris-virginica'] = 2\n",
    "\n",
    "# Seleccionar solo setosa (0) y virginica (2)\n",
    "mask = np.isin(y_numeric, [0, 2])\n",
    "X = X_raw[mask]\n",
    "y = y_numeric[mask]\n",
    "\n",
    "# Convertir virginica (2) a 1 para clasificación binaria\n",
    "y = (y == 2).astype(int)\n",
    "\n",
    "# Normalizar características manualmente\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X = (X - X_mean) / X_std\n",
    "\n",
    "# Verificar los datos procesados\n",
    "print(\"Forma de X:\", X.shape)\n",
    "print(\"Forma de y:\", y.shape)\n",
    "print(\"Valores únicos en y:\", np.unique(y))\n",
    "print(\"Distribución de clases:\", np.bincount(y))\n",
    "\n",
    "# División Hold-out (70/30)\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "split_point = int(len(X) * 0.7)\n",
    "\n",
    "X_train = X[indices[:split_point]]\n",
    "X_test = X[indices[split_point:]]\n",
    "y_train = y[indices[:split_point]]\n",
    "y_test = y[indices[split_point:]]\n",
    "\n",
    "# Verificar conjuntos de entrenamiento y prueba\n",
    "print(\"\\nConjuntos de entrenamiento y prueba:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Entrenar el perceptrón\n",
    "perceptron = SimplePerceptron(learning_rate=0.1, n_iterations=1000)\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_train_pred = perceptron.predict(X_train)\n",
    "y_test_pred = perceptron.predict(X_test)\n",
    "\n",
    "# Calcular y mostrar métricas para conjunto de entrenamiento\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred)\n",
    "print(\"\\nResultados en conjunto de entrenamiento:\")\n",
    "print(f\"Accuracy: {train_metrics[0]:.4f}\")\n",
    "print(f\"Precision: {train_metrics[1]:.4f}\")\n",
    "print(f\"Recall: {train_metrics[2]:.4f}\")\n",
    "print(f\"F1-Score: {train_metrics[3]:.4f}\")\n",
    "print(f\"Matriz de Confusión:\")\n",
    "print(f\"TP: {train_metrics[4]}, TN: {train_metrics[5]}\")\n",
    "print(f\"FP: {train_metrics[6]}, FN: {train_metrics[7]}\")\n",
    "\n",
    "# Calcular y mostrar métricas para conjunto de prueba\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred)\n",
    "print(\"\\nResultados en conjunto de prueba:\")\n",
    "print(f\"Accuracy: {test_metrics[0]:.4f}\")\n",
    "print(f\"Precision: {test_metrics[1]:.4f}\")\n",
    "print(f\"Recall: {test_metrics[2]:.4f}\")\n",
    "print(f\"F1-Score: {test_metrics[3]:.4f}\")\n",
    "print(f\"Matriz de Confusión:\")\n",
    "print(f\"TP: {test_metrics[4]}, TN: {test_metrics[5]}\")\n",
    "print(f\"FP: {test_metrics[6]}, FN: {test_metrics[7]}\")\n",
    "\n",
    "print(\"\\nPesos finales:\", perceptron.weights)\n",
    "print(\"Bias final:\", perceptron.bias)\n",
    "print(\"Iteraciones hasta convergencia:\", len(perceptron.errors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
